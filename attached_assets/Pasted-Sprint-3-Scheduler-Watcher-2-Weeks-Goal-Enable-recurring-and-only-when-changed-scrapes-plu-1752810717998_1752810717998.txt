Sprint‚ÄØ3: Scheduler & Watcher (2‚ÄØWeeks)

Goal: Enable recurring and ‚Äúonly‚Äêwhen‚Äêchanged‚Äù scrapes, plus webhook‚Äëtriggered jobs.
üìù User Stories

    Recurring Jobs

        As a user, I want to schedule scrapes via cron (e.g. every day at 3‚ÄØAM) so they run automatically.

    Change‚ÄëWatcher

        As a user, I want a ‚Äúwatch‚Äù mode that only runs the scrape when the start‚Äêpage content has changed.

    Webhook Triggers

        As a user, I want to POST to an API endpoint to kick off a scrape on‚Äëdemand.

    Developer Experience

        As a dev, I need a SchedulerService backed by BullMQ + Redis, and a WatcherService that hashes and compares content.

    CLI & API

        As a power‚Äëuser, I want CLI flags --cron and --watch and a clippr schedule:webhook command.

        As an integrator, I want a POST /api/scrape endpoint accepting schedule/watch config.

‚úî Acceptance Criteria

    clippr schedule --cron "0 3 * * *" --watch registers a recurring, watch‚Äêmode job in Redis/BullMQ.

    Jobs fire at correct times (verify via logs) and skip if no change detected.

    clippr run --webhook-id myhook immediately enqueues the job tied to myhook.

    REST API: POST /api/scrape with JSON {‚Ä¶scheduleConfig‚Ä¶} enqueues appropriately.

    Unit tests for watcher hashing logic and scheduler registration.

    Docker image includes redis host config.

üìÇ File Structure & Key Files

/api
 ‚îî‚îÄ src
    ‚îî‚îÄ engine
       ‚îú‚îÄ types.ts           (add ScheduleConfig)
       ‚îú‚îÄ ProxyManager.ts    (unchanged)
       ‚îú‚îÄ HTTPWorker.ts      (unchanged)
       ‚îú‚îÄ PuppeteerWorker.ts (unchanged)
       ‚îú‚îÄ ScraperService.ts  (invoke watcher + scheduling)
       ‚îú‚îÄ SchedulerService.ts
       ‚îî‚îÄ WatcherService.ts

 ‚îî‚îÄ server
    ‚îî‚îÄ scrape.routes.ts     (add webhook POST)

/cli
 ‚îî‚îÄ src
    ‚îî‚îÄ commands
       ‚îú‚îÄ scrape.ts         (add --cron/--watch flags)
       ‚îú‚îÄ schedule.ts       (new command)
       ‚îî‚îÄ run-webhook.ts    (new command)

/tests
 ‚îî‚îÄ engine
    ‚îú‚îÄ WatcherService.test.ts
    ‚îî‚îÄ SchedulerService.test.ts

1) types.ts

// api/src/engine/types.ts

export interface ScheduleConfig {
  /** Cron expr for recurring jobs (e.g. "0 3 * * *") */
  cron?: string;
  /** Only run when content changes */
  watch?: boolean;
  /** Optional CSS/XPath selector for partial watching */
  watchSelector?: string;
}

export interface ProxyConfig { /* unchanged */ }
export interface JobConfig {
  startUrl: string;
  depth: number;
  include: string[];
  renderMode: 'http'|'puppeteer';
  concurrency: number;
  proxyConfig: ProxyConfig;
  scheduleConfig?: ScheduleConfig;
}

export interface Job { url: string }
export interface WorkerInterface {
  fetch(job: Job): Promise<string>;
  close?(): Promise<void>;
}

2) WatcherService.ts

// api/src/engine/WatcherService.ts
import Redis from 'ioredis';
import fetch from 'node-fetch';
import crypto from 'crypto';
import { ScheduleConfig } from './types.js';

const redis = new Redis({
  host: process.env.REDIS_HOST || '127.0.0.1',
  port: Number(process.env.REDIS_PORT || 6379)
});

/**
 * Returns true if content changed (or first run),
 * false if unchanged and watch=true.
 */
export async function shouldRunWatch(
  url: string,
  scheduleConfig: ScheduleConfig
): Promise<boolean> {
  // 1) fetch raw content
  const res = await fetch(url, { timeout: 15000 });
  const text = await res.text();

  // 2) optionally extract with selector (not implemented here)
  //    if scheduleConfig.watchSelector, parse HTML and extract that element

  // 3) compute hash
  const hash = crypto.createHash('sha256').update(text).digest('hex');
  const key = `watcher:${url}`;

  // 4) compare to previous
  const prev = await redis.get(key);
  if (prev === hash) return false;

  // 5) store new hash
  await redis.set(key, hash);
  return true;
}

3) SchedulerService.ts

// api/src/engine/SchedulerService.ts
import { Queue, Worker, QueueScheduler, RepeatOptions } from 'bullmq';
import { ScraperService } from './ScraperService.js';
import { JobConfig } from './types.js';

const connection = {
  host: process.env.REDIS_HOST || '127.0.0.1',
  port: Number(process.env.REDIS_PORT || 6379)
};
const queueName = 'scrape-jobs';

// 1) Initialize queue + scheduler
export const scrapeQueue = new Queue(queueName, { connection });
new QueueScheduler(queueName, { connection });

// 2) Enqueue a one‚Äëoff or recurring job
export async function scheduleJob(
  jobId: string,
  cfg: JobConfig,
  cron?: string
) {
  const opts: RepeatOptions = cron
    ? { cron, tz: process.env.TZ || 'UTC' }
    : {};

  await scrapeQueue.add(jobId, cfg, { repeat: opts, removeOnComplete: true });
}

// 3) Worker to process jobs
new Worker(queueName, async job => {
  const cfg = job.data as JobConfig;
  console.log(`‚ñ∂ Running job ${job.id} startUrl=${cfg.startUrl}`);

  // 1) watch logic
  if (cfg.scheduleConfig?.watch) {
    const { shouldRunWatch } = await import('./WatcherService.js');
    const ok = await shouldRunWatch(cfg.startUrl, cfg.scheduleConfig);
    if (!ok) {
      console.log(`‚Ü© Skipping ${cfg.startUrl} (no change)`);
      return;
    }
  }

  // 2) run the scrape
  await ScraperService.runJob(cfg);
}, { connection });

// 4) Optional event hooks
scrapeQueue.on('completed', job => console.log(`‚úî Job ${job.id} completed`));
scrapeQueue.on('failed', (job, err) =>
  console.error(`‚úñ Job ${job?.id} failed:`, err)
);

4) Scrape Routes (Webhook)

// api/src/server/scrape.routes.ts
import { Router } from 'express';
import { scheduleJob, scrapeQueue } from '../engine/SchedulerService.js';
import { JobConfig } from '../engine/types.js';
import { v4 as uuidv4 } from 'uuid';

const router = Router();

/** 
 * POST /api/scrape
 * body: {
 *   jobConfig: JobConfig,
 *   scheduleConfig?: ScheduleConfig,
 *   webhookId?: string
 * }
 */
router.post('/', async (req, res) => {
  const { jobConfig, scheduleConfig } = req.body as {
    jobConfig: JobConfig;
    scheduleConfig?: JobConfig['scheduleConfig'];
  };
  const jobId = 'hook:' + uuidv4();

  // 1) if cron or watch specified ‚Üí schedule
  if (scheduleConfig?.cron || scheduleConfig?.watch) {
    jobConfig.scheduleConfig = scheduleConfig;
    await scheduleJob(jobId, jobConfig, scheduleConfig.cron);
    return res.json({ jobId, status: 'scheduled' });
  }

  // 2) else ‚Üí enqueue one‚Äëoff
  await scrapeQueue.add(jobId, jobConfig);
  return res.json({ jobId, status: 'queued' });
});

export default router;

5) CLI Commands
a) Add flags to scrape.ts

// cli/src/commands/scrape.ts
// ... existing imports ...
program
  .requiredOption('--start-url <url>')
  // ...
  .option('--cron <expr>', 'cron expression for recurring')
  .option('--watch', 'only when content changes')
  // ...
  .action(async opts => {
    const scheduleConfig = {
      cron: opts.cron,
      watch: Boolean(opts.watch)
    };
    const cfg: JobConfig = {
      // ... other opts ...
      scheduleConfig
    };
    // Direct enqueue if no cron/watch:
    if (!opts.cron && !opts.watch) {
      await scrapeQueue.add('cli:'+Date.now(), cfg);
      console.log('‚úî Job queued');
    } else {
      await scheduleJob('cli:'+Date.now(), cfg, opts.cron);
      console.log(`‚úî Job scheduled cron=${opts.cron} watch=${opts.watch}`);
    }
  });

b) New schedule command (optional)

// cli/src/commands/schedule.ts
import { Command } from 'commander';
import { scheduleJob } from '../../api/dist/engine/SchedulerService.js';
import { JobConfig } from '../../api/dist/engine/types.js';

export const scheduleCmd = new Command('schedule')
  .description('Schedule a recurring scrape')
  .requiredOption('--start-url <url>')
  .requiredOption('--cron <expr>')
  .option('--watch', 'only when content changes')
  // ... include depth/include/proxy flags ...
  .action(async opts => {
    const cfg: JobConfig = { /* build from opts */ };
    cfg.scheduleConfig = { cron: opts.cron, watch: !!opts.watch };
    await scheduleJob('cli:'+Date.now(), cfg, opts.cron);
    console.log('‚úî Scheduled');
  });

6) Tests
WatcherService.test.ts

// tests/engine/WatcherService.test.ts
import http from 'http';
import fetch from 'node-fetch';
import { shouldRunWatch } from '../../api/src/engine/WatcherService.js';

describe('WatcherService', () => {
  let server: http.Server, port: number;
  let calls = 0;
  beforeAll(done => {
    server = http.createServer((_, res) => {
      calls++;
      res.end(`<html>${calls}</html>`);
    }).listen(0, () => {
      // @ts-ignore
      port = server.address().port;
      done();
    });
  });
  afterAll(done => server.close(done));

  it('runs on first fetch, skips if unchanged', async () => {
    const url = `http://localhost:${port}`;
    const cfg = { watch: true };
    expect(await shouldRunWatch(url, cfg)).toBe(true);
    // same content on second call? Should skip
    expect(await shouldRunWatch(url, cfg)).toBe(false);
  });
});

SchedulerService.test.ts

// tests/engine/SchedulerService.test.ts
import { scrapeQueue, scheduleJob } from '../../api/src/engine/SchedulerService.js';
import { JobConfig } from '../../api/src/engine/types.js';

describe('SchedulerService', () => {
  it('schedules a cron job', async () => {
    const cfg = {
      startUrl: 'http://example.com',
      depth: 1,
      include: ['html'],
      renderMode: 'http',
      concurrency: 1,
      proxyConfig: { mode: 'none' }
    } as JobConfig;
    await scheduleJob('test-job', cfg, '*/5 * * * *');
    const jobs = await scrapeQueue.getRepeatableJobs();
    const found = jobs.find(j => j.name === 'test-job');
    expect(found).toBeDefined();
  });
});

‚úÖ Next Steps

    Merge these files into /api and /cli.

    Install BullMQ & ioredis:

cd api
npm install bullmq ioredis

Ensure REDIS_HOST & REDIS_PORT in your .env.

Run Tests: npm test in /api.

Start API & Worker:

# in one shell:
npm run dev --prefix api   # starts Express + SchedulerService worker
# in another:
clippr schedule --start-url https://example.com --cron "*/1 * * * *" --watch

Observe logs every minute; content changes trigger scrapes, otherwise skipped.