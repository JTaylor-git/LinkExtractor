**You are a full‑stack engineering team tasked with building “Clippy,” a next‑gen, multi‑level, template‑driven web scraping platform with both No‑Code and Pro‑Code UX. Deliver production‑ready code only—no placeholders or half‑baked stubs.**

Phase 1

### 1. Core Philosophy

1. **No‑Code & Pro‑Code in One**

   * **Beginners**: point‑and‑click “Site + Targets” wizard in the UI.
   * **Power users**: ability to drop in custom JS/Puppeteer snippets, CSS/XPath selectors, or shell‑style command scripts.

2. **Modular & Extensible**

   * Plugin marketplace for authentication (OAuth, SAML, cookie‑jar), proxy integrations, export formats, parsers.
   * Open REST & gRPC API so external systems or custom front‑ends can trigger scrapes, poll status, and fetch results.

3. **Global Overview + Deep Dive**

   * Dark‑mode, Shodan‑inspired “Globe Dashboard” with 3D rotatable Earth showing active scrapes by region, domain clusters, or resource types.
   * Drill‑in timeline views, error heatmaps, HTTP request logs.

---

### 2. Feature Requirements

A. Project & Task Management

* Multi‑site Projects: group scrapes, reuse target lists, share templates.
* Crawl Settings UI: depth slider, link‑follow rules, domain restriction toggles.
* Scheduler & Watcher: recurring jobs, “only when page updates,” webhook‑triggered tasks.

B. Smart Templates & AI Assist

* Template Library: pre‑built templates for e‑commerce, job boards, maps, PDFs, social media feeds.
* One‑click “Import from Crawlbase/Scrapingbee/Zyte” to reverse‑engineer endpoints.
* AI Assistant: auto‑detect tables, price blocks, social posts, geo‑coordinates; live preview record extraction as you hover.

C. Scraping Engine

* **Hybrid Rendering**: fast HTTP workers + optional Headless Chrome/Puppeteer for JS‑heavy pages.
* **Distributed Proxies**: native integration with Crawlbase, ScraperAPI, or pluggable custom proxy pools.
* **Rate & Bandwidth Controls**: per‑domain throttling, global QPS caps, concurrency dials.
* **Authentication Modules**: cookie‑jar replay, OAuth flows, form‑login recorder, 2FA support.

D. Data Delivery

* **Export Formats**: JSON, CSV/TSV, Excel, XML, whole‑site mirror, image/PDF zip, KML/GeoJSON.
* **Destinations**: Local disk, S3/GCS, FTP/SFTP, Google Sheets, PostgreSQL/MongoDB insert, webhooks.
* **Live QA**: data table viewer with filtering, dedupe/validator rules, raw file browser.

E. CLI & API

* **CLI** (TypeScript or Go), installable via npm/brew:

  ```bash
  # Example usage
  suckerfish scrape \
    --project ecommerce-wh \
    --targets ".product-item img,.price:text" \
    --depth 3 \
    --concurrency 10 \
    --include geojson,kml \
    --output ./results.json
  ```
* **REST & gRPC**: endpoints for project creation, task submission, status polling, result download.
* **Authentication**: API key‑based with role scopes.

---

 3. UI/UX Specification

* **Global Theme**: Dark blue/navy background, neon‑accent highlights. Responsive design for desktop and tablet.

* **Left Nav** (vertical):

  * Dashboard (Globe)
  * Projects
  * Templates
  * Logs
  * Settings

* **Dashboard**:

  * Rotatable 3D globe with pin drops
  * Top‑bar counters: Active | Queued | Failed | Live QPS gauge

* **Project View**: Split panel:

  * **Upper Pane**: Task list with status badges (✓, ⚠, ✖)
  * **Detail Pane**:

    * **Flow Editor**: Drag‑drop nodes (Fetch → Render → Parse → Export)
    * **Settings**: Depth slider, concurrency dial, proxy dropdown
    * **Live Console**: real‑time HTTP logs, JS errors, timing

* **Template Gallery**: Card grid with categories (Products, Maps, Social), each card shows example JSON schema, “Clone & Customize” CTA.

* **Analytics Tab**:

  * Charts: requests/time, per‑domain latency, resource‑type pie chart, URL‑pattern error heatmap.

---

 4. System Architecture

```
┌──────────────┐        ┌───────────────┐       ┌──────────┐
│  Frontend    │ <–REST–>│ Backend API   │<–ORM–>│ Database │
│ (SvelteKit/  │        │ (Node.js /    │       │(Postgres)│
│  React+TS)   │        │  NestJS/Express)      └──────────┘
└──────────────┘         └───────────────┘
        ▲                       │
        │                       ▼
        │              ┌─────────────────┐
        │              │ Scraping Engine │
        │              │ • HTTP Workers  │
        │              │ • Puppeteer Core│
        │              │ • Proxy Manager │
        │              └─────────────────┘
        │                       │
        └── Scheduler & Queue ──┘
            (BullMQ / RabbitMQ)
                │
                ▼
         ┌─────────────┐
         │ Storage     │
         │ • S3 / GCS  │
         │ • Redis     │
         │ • Local FS  │
         └─────────────┘
```

---

5. BlueGolf Geo‑Data Example

**Developer Script** – provide full, copy‑pastable Python integration with the CLI:

```bash
# brew install suckerfish
export SUCKERFISH_API_KEY="sk_…"

# Create project
suckerfish project create \
  --api-key "$SUCKERFISH_API_KEY" \
  --name "BlueGolf GeoScrape"

# Run scrape
suckerfish scrape \
  --api-key "$SUCKERFISH_API_KEY" \
  --project "BlueGolf GeoScrape" \
  --start-url "https://course.bluegolf.com/bluegolf/course/clist/alist/index.htm" \
  --depth 2 \
  --concurrency 5 \
  --include geojson,kml \
  --output ./bluegolf_data/manifest.json
```

**No‑Code UI Steps** – mirror above CLI settings in the point‑and‑click wizard, live‑preview selectors, then export ZIP/S3.

---

6. Deliverables & Expectations

1. **Code Quality**: modular, well‑documented, unit/integration tests.
2. **Full Implementation**: frontend components (SvelteKit or React+TS), backend services (Node.js), CLI tool, database schema, Dockerfiles, CI/CD manifests.
3. **Responsive & Accessible**: mobile/tablet support, WCAG AA.
4. **Security & Scalability**: API key auth, role scopes, proxy rotation, queue scaling.
5. **No placeholders**: every component, config, and integration must be functional out‑of‑the‑box.

Begin by outlining the repo structure, tech stack choices, and a phased implementation plan. Then scaffold the core CLI, backend API, and UI shell.

**You are a full‑stack engineering team tasked with building “Clippy,” a next‑gen, multi‑level, template‑driven web scraping platform with both No‑Code and Pro‑Code UX. Deliver production‑ready code only—no placeholders or half‑baked stubs.**


### 1. Core Philosophy

1. **No‑Code & Pro‑Code in One**

   * **Beginners**: point‑and‑click “Site + Targets” wizard in the UI.
   * **Power users**: ability to drop in custom JS/Puppeteer snippets, CSS/XPath selectors, or shell‑style command scripts.

2. **Modular & Extensible**

   * Plugin marketplace for authentication (OAuth, SAML, cookie‑jar), proxy integrations, export formats, parsers.
   * Open REST & gRPC API so external systems or custom front‑ends can trigger scrapes, poll status, and fetch results.

3. **Global Overview + Deep Dive**

   * Dark‑mode, Shodan‑inspired “Globe Dashboard” with 3D rotatable Earth showing active scrapes by region, domain clusters, or resource types.
   * Drill‑in timeline views, error heatmaps, HTTP request logs.

---

### 2. Feature Requirements

A. Project & Task Management

* Multi‑site Projects: group scrapes, reuse target lists, share templates.
* Crawl Settings UI: depth slider, link‑follow rules, domain restriction toggles.
* Scheduler & Watcher: recurring jobs, “only when page updates,” webhook‑triggered tasks.

B. Smart Templates & AI Assist

* Template Library: pre‑built templates for e‑commerce, job boards, maps, PDFs, social media feeds.
* One‑click “Import from Crawlbase/Scrapingbee/Zyte” to reverse‑engineer endpoints.
* AI Assistant: auto‑detect tables, price blocks, social posts, geo‑coordinates; live preview record extraction as you hover.

C. Scraping Engine

* **Hybrid Rendering**: fast HTTP workers + optional Headless Chrome/Puppeteer for JS‑heavy pages.
* **Distributed Proxies**: native integration with Crawlbase, ScraperAPI, or pluggable custom proxy pools.
* **Rate & Bandwidth Controls**: per‑domain throttling, global QPS caps, concurrency dials.
* **Authentication Modules**: cookie‑jar replay, OAuth flows, form‑login recorder, 2FA support.

D. Data Delivery

* **Export Formats**: JSON, CSV/TSV, Excel, XML, whole‑site mirror, image/PDF zip, KML/GeoJSON.
* **Destinations**: Local disk, S3/GCS, FTP/SFTP, Google Sheets, PostgreSQL/MongoDB insert, webhooks.
* **Live QA**: data table viewer with filtering, dedupe/validator rules, raw file browser.

E. CLI & API

* **CLI** (TypeScript or Go), installable via npm/brew:

  ```bash
  # Example usage
  suckerfish scrape \
    --project ecommerce-wh \
    --targets ".product-item img,.price:text" \
    --depth 3 \
    --concurrency 10 \
    --include geojson,kml \
    --output ./results.json
  ```
* **REST & gRPC**: endpoints for project creation, task submission, status polling, result download.
* **Authentication**: API key‑based with role scopes.

---

 3. UI/UX Specification

* **Global Theme**: Dark blue/navy background, neon‑accent highlights. Responsive design for desktop and tablet.

* **Left Nav** (vertical):

  * Dashboard (Globe)
  * Projects
  * Templates
  * Logs
  * Settings

* **Dashboard**:

  * Rotatable 3D globe with pin drops
  * Top‑bar counters: Active | Queued | Failed | Live QPS gauge

* **Project View**: Split panel:

  * **Upper Pane**: Task list with status badges (✓, ⚠, ✖)
  * **Detail Pane**:

    * **Flow Editor**: Drag‑drop nodes (Fetch → Render → Parse → Export)
    * **Settings**: Depth slider, concurrency dial, proxy dropdown
    * **Live Console**: real‑time HTTP logs, JS errors, timing

* **Template Gallery**: Card grid with categories (Products, Maps, Social), each card shows example JSON schema, “Clone & Customize” CTA.

* **Analytics Tab**:

  * Charts: requests/time, per‑domain latency, resource‑type pie chart, URL‑pattern error heatmap.

---

 4. System Architecture

```
┌──────────────┐        ┌───────────────┐       ┌──────────┐
│  Frontend    │ <–REST–>│ Backend API   │<–ORM–>│ Database │
│ (SvelteKit/  │        │ (Node.js /    │       │(Postgres)│
│  React+TS)   │        │  NestJS/Express)      └──────────┘
└──────────────┘         └───────────────┘
        ▲                       │
        │                       ▼
        │              ┌─────────────────┐
        │              │ Scraping Engine │
        │              │ • HTTP Workers  │
        │              │ • Puppeteer Core│
        │              │ • Proxy Manager │
        │              └─────────────────┘
        │                       │
        └── Scheduler & Queue ──┘
            (BullMQ / RabbitMQ)
                │
                ▼
         ┌─────────────┐
         │ Storage     │
         │ • S3 / GCS  │
         │ • Redis     │
         │ • Local FS  │
         └─────────────┘
```

---

5. BlueGolf Geo‑Data Example

**Developer Script** – provide full, copy‑pastable Python integration with the CLI:

```bash
# brew install suckerfish
export SUCKERFISH_API_KEY="sk_…"

# Create project
suckerfish project create \
  --api-key "$SUCKERFISH_API_KEY" \
  --name "BlueGolf GeoScrape"

# Run scrape
suckerfish scrape \
  --api-key "$SUCKERFISH_API_KEY" \
  --project "BlueGolf GeoScrape" \
  --start-url "https://course.bluegolf.com/bluegolf/course/clist/alist/index.htm" \
  --depth 2 \
  --concurrency 5 \
  --include geojson,kml \
  --output ./bluegolf_data/manifest.json
```

**No‑Code UI Steps** – mirror above CLI settings in the point‑and‑click wizard, live‑preview selectors, then export ZIP/S3.

---

6. Deliverables & Expectations

1. **Code Quality**: modular, well‑documented, unit/integration tests.
2. **Full Implementation**: frontend components (SvelteKit or React+TS), backend services (Node.js), CLI tool, database schema, Dockerfiles, CI/CD manifests.
3. **Responsive & Accessible**: mobile/tablet support, WCAG AA.
4. **Security & Scalability**: API key auth, role scopes, proxy rotation, queue scaling.
5. **No placeholders**: every component, config, and integration must be functional out‑of‑the‑box.

Begin by outlining the repo structure, tech stack choices, and a phased implementation plan. Then scaffold the core CLI, backend API, and UI shell.

7. “Clippr” UI Concept

* **Mascot**: Clippy sits in the bottom-right corner of every screen, animating and speaking helpful tips.
* **Theme**: Keep the dark-mode, neon accents, but add little pops of paperclip-silver and Clippy’s blue.
* **Interactions**:

  * **Onboarding**: Clippy waves and says “Let’s set up your first scrape!”
  * **Wizard Steps**: As you choose URL, depth, filters, Clippy pops up “Nice choice!” or “Don’t forget geo-files!”
  * **Errors**: Clippy facepalms on failures, then suggests “Try lowering your concurrency.”
  * **Success**: Clippy does a little dance when your job completes.

---

8. Front-End Integration (Svelte)

**Install ClippyJS**

```bash
npm install clippyjs
```

**Create `ClippyAssistant.svelte`**

```svelte
<script lang="ts">
  import { onMount, createEventDispatcher } from 'svelte';
  import clippy from 'clippyjs';

  const dispatch = createEventDispatcher();
  let agent: any;

  // Map wizard steps → hints
  const hints: Record<number,string> = {
    1: 'Enter the site URL above',
    2: 'Set depth & file filters',
    3: 'Click ▶︎ Run to start scraping',
    4: 'All done—download your data!'
  };

  onMount(() => {
    clippy.load('Clippy', (a: any) => {
      agent = a;
      agent.show();
      agent.moveTo(window.innerWidth - 200, window.innerHeight - 200);
      agent.speak('🔍 Hi! I’m Clippr, your scraping assistant.');
      // Listen for step changes
      dispatch('ready');
    });
    // Clean up on destroy
    return () => agent && agent.hide();
  });

  // Public method to advance Clippy’s hint
  export function stepTo(n: number) {
    if (!agent) return;
    const text = hints[n] || 'Let me know if you need help!';
    agent.animate();        // random animation
    agent.speak(text);
  }
</script>

<style>
  /* Clippy’s <div> is injected by clippyjs; we just ensure our wrapper sits atop */
  :global(body) { position: relative; }
</style>

<!-- no visible markup; Clippy attachments go to <body> -->
```

**Wire it up in your Page**

```svelte
<script lang="ts">
  import ClippyAssistant, { stepTo } from './ClippyAssistant.svelte';
  let wizardStep = 1;

  function nextStep() {
    wizardStep += 1;
    stepTo(wizardStep);
  }
</script>

<ClippyAssistant on:ready={() => stepTo(1)} />

<div class="wizard">
  {#if wizardStep === 1}
    <!-- Step 1 UI -->
    <button on:click={nextStep}>Set URL →</button>
  {:else if wizardStep === 2}
    <!-- Step 2 UI -->
    <button on:click={nextStep}>Set Depth & Filters →</button>
  {:else if wizardStep === 3}
    <!-- Step 3 UI -->
    <button on:click={nextStep}>Run Scrape →</button>
  {:else}
    <p>🎉 All steps done! Download your data.</p>
  {/if}
</div>
```

---

9. Updated Prompt for Your Coding Agent

 **Project:** Clippr – The Clippy-Powered Web Scraper

 **Goal:** Build a dual No-Code/Pro-Code scraping platform with Clippy as the interactive assistant.

 **Tech Stack:**

 * **Frontend:** SvelteKit + Tailwind (dark theme, neon accents)
 * **Backend:** Node.js + NestJS/Express + BullMQ
 * **Scraping Engine:** HTTP workers + Puppeteer-core + Proxy Manager
 * **CLI:** TypeScript, published via npm & Homebrew
 * **DB/Storage:** PostgreSQL (metadata), S3/GCS (files), Redis (cache)

  Must-Have Features

 1. **Clippy Assistant**

    * Svelte component wrapping `clippyjs`
    * `stepTo(n)` API for wizard hints, error & success animations
 2. **No-Code Wizard**

    * 4 steps (URL → Depth & Filters → Run → Download)
    * Live preview of selectors & file-type filters
    * Clippy hints at each step
 3. **Pro-Code Mode**

    * Embedded code editor for custom JS/Puppeteer snippets
    * CLI commands: `clippr project create`, `clippr scrape`, `clippr status`, `clippr download`
 4. **Dashboard**

    * Dark-mode globe, live QPS gauge, task list with badges
    * Clippy notifications for job events
 5. **API**

    * REST & gRPC: project/task CRUD, status polling, result fetching
    * API-key auth with scopes

 Deliverables

 * Full repo scaffold (monorepo with `/web`, `/api`, `/cli`)
 * `ClippyAssistant.svelte` + wiring in the wizard
 * CLI tool in TS, published config
 * Backend API modules, DB schema, sample Docker + CI pipelines
 * End-to-end tests for both No-Code and Pro-Code flows

 **Begin** by scaffolding the monorepo and implementing `ClippyAssistant.svelte` exactly as above—no placeholders. Ensure Clippy appears and speaks on load.
---