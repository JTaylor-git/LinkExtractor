**You are a fullâ€‘stack engineering team tasked with building â€œClippy,â€ a nextâ€‘gen, multiâ€‘level, templateâ€‘driven web scraping platform with both Noâ€‘Code and Proâ€‘Code UX. Deliver productionâ€‘ready code onlyâ€”no placeholders or halfâ€‘baked stubs.**

Phase 1

### 1. Core Philosophy

1. **Noâ€‘Code & Proâ€‘Code in One**

   * **Beginners**: pointâ€‘andâ€‘click â€œSite + Targetsâ€ wizard in the UI.
   * **Power users**: ability to drop in custom JS/Puppeteer snippets, CSS/XPath selectors, or shellâ€‘style command scripts.

2. **Modular & Extensible**

   * Plugin marketplace for authentication (OAuth, SAML, cookieâ€‘jar), proxy integrations, export formats, parsers.
   * Open REST & gRPC API so external systems or custom frontâ€‘ends can trigger scrapes, poll status, and fetch results.

3. **Global Overview + Deep Dive**

   * Darkâ€‘mode, Shodanâ€‘inspired â€œGlobe Dashboardâ€ with 3D rotatable Earth showing active scrapes by region, domain clusters, or resource types.
   * Drillâ€‘in timeline views, error heatmaps, HTTP request logs.

---

### 2. Feature Requirements

A. Project & Task Management

* Multiâ€‘site Projects: group scrapes, reuse target lists, share templates.
* Crawl Settings UI: depth slider, linkâ€‘follow rules, domain restriction toggles.
* Scheduler & Watcher: recurring jobs, â€œonly when page updates,â€ webhookâ€‘triggered tasks.

B. Smart Templates & AI Assist

* Template Library: preâ€‘built templates for eâ€‘commerce, job boards, maps, PDFs, social media feeds.
* Oneâ€‘click â€œImport from Crawlbase/Scrapingbee/Zyteâ€ to reverseâ€‘engineer endpoints.
* AI Assistant: autoâ€‘detect tables, price blocks, social posts, geoâ€‘coordinates; live preview record extraction as you hover.

C. Scraping Engine

* **Hybrid Rendering**: fast HTTP workers + optional Headless Chrome/Puppeteer for JSâ€‘heavy pages.
* **Distributed Proxies**: native integration with Crawlbase, ScraperAPI, or pluggable custom proxy pools.
* **Rate & Bandwidth Controls**: perâ€‘domain throttling, global QPS caps, concurrency dials.
* **Authentication Modules**: cookieâ€‘jar replay, OAuth flows, formâ€‘login recorder, 2FA support.

D. Data Delivery

* **Export Formats**: JSON, CSV/TSV, Excel, XML, wholeâ€‘site mirror, image/PDF zip, KML/GeoJSON.
* **Destinations**: Local disk, S3/GCS, FTP/SFTP, Google Sheets, PostgreSQL/MongoDB insert, webhooks.
* **Live QA**: data table viewer with filtering, dedupe/validator rules, raw file browser.

E. CLI & API

* **CLI** (TypeScript or Go), installable via npm/brew:

  ```bash
  # Example usage
  suckerfish scrape \
    --project ecommerce-wh \
    --targets ".product-item img,.price:text" \
    --depth 3 \
    --concurrency 10 \
    --include geojson,kml \
    --output ./results.json
  ```
* **REST & gRPC**: endpoints for project creation, task submission, status polling, result download.
* **Authentication**: API keyâ€‘based with role scopes.

---

 3. UI/UX Specification

* **Global Theme**: Dark blue/navy background, neonâ€‘accent highlights. Responsive design for desktop and tablet.

* **Left Nav** (vertical):

  * Dashboard (Globe)
  * Projects
  * Templates
  * Logs
  * Settings

* **Dashboard**:

  * Rotatable 3D globe with pin drops
  * Topâ€‘bar counters: Active | Queued | Failed | Live QPS gauge

* **Project View**: Split panel:

  * **Upper Pane**: Task list with status badges (âœ“, âš , âœ–)
  * **Detail Pane**:

    * **Flow Editor**: Dragâ€‘drop nodes (Fetch â†’ Render â†’ Parse â†’ Export)
    * **Settings**: Depth slider, concurrency dial, proxy dropdown
    * **Live Console**: realâ€‘time HTTP logs, JS errors, timing

* **Template Gallery**: Card grid with categories (Products, Maps, Social), each card shows example JSON schema, â€œClone & Customizeâ€ CTA.

* **Analytics Tab**:

  * Charts: requests/time, perâ€‘domain latency, resourceâ€‘type pie chart, URLâ€‘pattern error heatmap.

---

 4. System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend    â”‚ <â€“RESTâ€“>â”‚ Backend API   â”‚<â€“ORMâ€“>â”‚ Database â”‚
â”‚ (SvelteKit/  â”‚        â”‚ (Node.js /    â”‚       â”‚(Postgres)â”‚
â”‚  React+TS)   â”‚        â”‚  NestJS/Express)      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                       â”‚
        â”‚                       â–¼
        â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚ Scraping Engine â”‚
        â”‚              â”‚ â€¢ HTTP Workers  â”‚
        â”‚              â”‚ â€¢ Puppeteer Coreâ”‚
        â”‚              â”‚ â€¢ Proxy Manager â”‚
        â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚
        â””â”€â”€ Scheduler & Queue â”€â”€â”˜
            (BullMQ / RabbitMQ)
                â”‚
                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Storage     â”‚
         â”‚ â€¢ S3 / GCS  â”‚
         â”‚ â€¢ Redis     â”‚
         â”‚ â€¢ Local FS  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

5. BlueGolf Geoâ€‘Data Example

**Developer Script** â€“ provide full, copyâ€‘pastable Python integration with the CLI:

```bash
# brew install suckerfish
export SUCKERFISH_API_KEY="sk_â€¦"

# Create project
suckerfish project create \
  --api-key "$SUCKERFISH_API_KEY" \
  --name "BlueGolf GeoScrape"

# Run scrape
suckerfish scrape \
  --api-key "$SUCKERFISH_API_KEY" \
  --project "BlueGolf GeoScrape" \
  --start-url "https://course.bluegolf.com/bluegolf/course/clist/alist/index.htm" \
  --depth 2 \
  --concurrency 5 \
  --include geojson,kml \
  --output ./bluegolf_data/manifest.json
```

**Noâ€‘Code UI Steps** â€“ mirror above CLI settings in the pointâ€‘andâ€‘click wizard, liveâ€‘preview selectors, then export ZIP/S3.

---

6. Deliverables & Expectations

1. **Code Quality**: modular, wellâ€‘documented, unit/integration tests.
2. **Full Implementation**: frontend components (SvelteKit or React+TS), backend services (Node.js), CLI tool, database schema, Dockerfiles, CI/CD manifests.
3. **Responsive & Accessible**: mobile/tablet support, WCAG AA.
4. **Security & Scalability**: API key auth, role scopes, proxy rotation, queue scaling.
5. **No placeholders**: every component, config, and integration must be functional outâ€‘ofâ€‘theâ€‘box.

Begin by outlining the repo structure, tech stack choices, and a phased implementation plan. Then scaffold the core CLI, backend API, and UI shell.

**You are a fullâ€‘stack engineering team tasked with building â€œClippy,â€ a nextâ€‘gen, multiâ€‘level, templateâ€‘driven web scraping platform with both Noâ€‘Code and Proâ€‘Code UX. Deliver productionâ€‘ready code onlyâ€”no placeholders or halfâ€‘baked stubs.**


### 1. Core Philosophy

1. **Noâ€‘Code & Proâ€‘Code in One**

   * **Beginners**: pointâ€‘andâ€‘click â€œSite + Targetsâ€ wizard in the UI.
   * **Power users**: ability to drop in custom JS/Puppeteer snippets, CSS/XPath selectors, or shellâ€‘style command scripts.

2. **Modular & Extensible**

   * Plugin marketplace for authentication (OAuth, SAML, cookieâ€‘jar), proxy integrations, export formats, parsers.
   * Open REST & gRPC API so external systems or custom frontâ€‘ends can trigger scrapes, poll status, and fetch results.

3. **Global Overview + Deep Dive**

   * Darkâ€‘mode, Shodanâ€‘inspired â€œGlobe Dashboardâ€ with 3D rotatable Earth showing active scrapes by region, domain clusters, or resource types.
   * Drillâ€‘in timeline views, error heatmaps, HTTP request logs.

---

### 2. Feature Requirements

A. Project & Task Management

* Multiâ€‘site Projects: group scrapes, reuse target lists, share templates.
* Crawl Settings UI: depth slider, linkâ€‘follow rules, domain restriction toggles.
* Scheduler & Watcher: recurring jobs, â€œonly when page updates,â€ webhookâ€‘triggered tasks.

B. Smart Templates & AI Assist

* Template Library: preâ€‘built templates for eâ€‘commerce, job boards, maps, PDFs, social media feeds.
* Oneâ€‘click â€œImport from Crawlbase/Scrapingbee/Zyteâ€ to reverseâ€‘engineer endpoints.
* AI Assistant: autoâ€‘detect tables, price blocks, social posts, geoâ€‘coordinates; live preview record extraction as you hover.

C. Scraping Engine

* **Hybrid Rendering**: fast HTTP workers + optional Headless Chrome/Puppeteer for JSâ€‘heavy pages.
* **Distributed Proxies**: native integration with Crawlbase, ScraperAPI, or pluggable custom proxy pools.
* **Rate & Bandwidth Controls**: perâ€‘domain throttling, global QPS caps, concurrency dials.
* **Authentication Modules**: cookieâ€‘jar replay, OAuth flows, formâ€‘login recorder, 2FA support.

D. Data Delivery

* **Export Formats**: JSON, CSV/TSV, Excel, XML, wholeâ€‘site mirror, image/PDF zip, KML/GeoJSON.
* **Destinations**: Local disk, S3/GCS, FTP/SFTP, Google Sheets, PostgreSQL/MongoDB insert, webhooks.
* **Live QA**: data table viewer with filtering, dedupe/validator rules, raw file browser.

E. CLI & API

* **CLI** (TypeScript or Go), installable via npm/brew:

  ```bash
  # Example usage
  suckerfish scrape \
    --project ecommerce-wh \
    --targets ".product-item img,.price:text" \
    --depth 3 \
    --concurrency 10 \
    --include geojson,kml \
    --output ./results.json
  ```
* **REST & gRPC**: endpoints for project creation, task submission, status polling, result download.
* **Authentication**: API keyâ€‘based with role scopes.

---

 3. UI/UX Specification

* **Global Theme**: Dark blue/navy background, neonâ€‘accent highlights. Responsive design for desktop and tablet.

* **Left Nav** (vertical):

  * Dashboard (Globe)
  * Projects
  * Templates
  * Logs
  * Settings

* **Dashboard**:

  * Rotatable 3D globe with pin drops
  * Topâ€‘bar counters: Active | Queued | Failed | Live QPS gauge

* **Project View**: Split panel:

  * **Upper Pane**: Task list with status badges (âœ“, âš , âœ–)
  * **Detail Pane**:

    * **Flow Editor**: Dragâ€‘drop nodes (Fetch â†’ Render â†’ Parse â†’ Export)
    * **Settings**: Depth slider, concurrency dial, proxy dropdown
    * **Live Console**: realâ€‘time HTTP logs, JS errors, timing

* **Template Gallery**: Card grid with categories (Products, Maps, Social), each card shows example JSON schema, â€œClone & Customizeâ€ CTA.

* **Analytics Tab**:

  * Charts: requests/time, perâ€‘domain latency, resourceâ€‘type pie chart, URLâ€‘pattern error heatmap.

---

 4. System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend    â”‚ <â€“RESTâ€“>â”‚ Backend API   â”‚<â€“ORMâ€“>â”‚ Database â”‚
â”‚ (SvelteKit/  â”‚        â”‚ (Node.js /    â”‚       â”‚(Postgres)â”‚
â”‚  React+TS)   â”‚        â”‚  NestJS/Express)      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                       â”‚
        â”‚                       â–¼
        â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚ Scraping Engine â”‚
        â”‚              â”‚ â€¢ HTTP Workers  â”‚
        â”‚              â”‚ â€¢ Puppeteer Coreâ”‚
        â”‚              â”‚ â€¢ Proxy Manager â”‚
        â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚
        â””â”€â”€ Scheduler & Queue â”€â”€â”˜
            (BullMQ / RabbitMQ)
                â”‚
                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Storage     â”‚
         â”‚ â€¢ S3 / GCS  â”‚
         â”‚ â€¢ Redis     â”‚
         â”‚ â€¢ Local FS  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

5. BlueGolf Geoâ€‘Data Example

**Developer Script** â€“ provide full, copyâ€‘pastable Python integration with the CLI:

```bash
# brew install suckerfish
export SUCKERFISH_API_KEY="sk_â€¦"

# Create project
suckerfish project create \
  --api-key "$SUCKERFISH_API_KEY" \
  --name "BlueGolf GeoScrape"

# Run scrape
suckerfish scrape \
  --api-key "$SUCKERFISH_API_KEY" \
  --project "BlueGolf GeoScrape" \
  --start-url "https://course.bluegolf.com/bluegolf/course/clist/alist/index.htm" \
  --depth 2 \
  --concurrency 5 \
  --include geojson,kml \
  --output ./bluegolf_data/manifest.json
```

**Noâ€‘Code UI Steps** â€“ mirror above CLI settings in the pointâ€‘andâ€‘click wizard, liveâ€‘preview selectors, then export ZIP/S3.

---

6. Deliverables & Expectations

1. **Code Quality**: modular, wellâ€‘documented, unit/integration tests.
2. **Full Implementation**: frontend components (SvelteKit or React+TS), backend services (Node.js), CLI tool, database schema, Dockerfiles, CI/CD manifests.
3. **Responsive & Accessible**: mobile/tablet support, WCAG AA.
4. **Security & Scalability**: API key auth, role scopes, proxy rotation, queue scaling.
5. **No placeholders**: every component, config, and integration must be functional outâ€‘ofâ€‘theâ€‘box.

Begin by outlining the repo structure, tech stack choices, and a phased implementation plan. Then scaffold the core CLI, backend API, and UI shell.

7. â€œClipprâ€ UI Concept

* **Mascot**: Clippy sits in the bottom-right corner of every screen, animating and speaking helpful tips.
* **Theme**: Keep the dark-mode, neon accents, but add little pops of paperclip-silver and Clippyâ€™s blue.
* **Interactions**:

  * **Onboarding**: Clippy waves and says â€œLetâ€™s set up your first scrape!â€
  * **Wizard Steps**: As you choose URL, depth, filters, Clippy pops up â€œNice choice!â€ or â€œDonâ€™t forget geo-files!â€
  * **Errors**: Clippy facepalms on failures, then suggests â€œTry lowering your concurrency.â€
  * **Success**: Clippy does a little dance when your job completes.

---

8. Front-End Integration (Svelte)

**Install ClippyJS**

```bash
npm install clippyjs
```

**Create `ClippyAssistant.svelte`**

```svelte
<script lang="ts">
  import { onMount, createEventDispatcher } from 'svelte';
  import clippy from 'clippyjs';

  const dispatch = createEventDispatcher();
  let agent: any;

  // Map wizard steps â†’ hints
  const hints: Record<number,string> = {
    1: 'Enter the site URL above',
    2: 'Set depth & file filters',
    3: 'Click â–¶ï¸ Run to start scraping',
    4: 'All doneâ€”download your data!'
  };

  onMount(() => {
    clippy.load('Clippy', (a: any) => {
      agent = a;
      agent.show();
      agent.moveTo(window.innerWidth - 200, window.innerHeight - 200);
      agent.speak('ğŸ” Hi! Iâ€™m Clippr, your scraping assistant.');
      // Listen for step changes
      dispatch('ready');
    });
    // Clean up on destroy
    return () => agent && agent.hide();
  });

  // Public method to advance Clippyâ€™s hint
  export function stepTo(n: number) {
    if (!agent) return;
    const text = hints[n] || 'Let me know if you need help!';
    agent.animate();        // random animation
    agent.speak(text);
  }
</script>

<style>
  /* Clippyâ€™s <div> is injected by clippyjs; we just ensure our wrapper sits atop */
  :global(body) { position: relative; }
</style>

<!-- no visible markup; Clippy attachments go to <body> -->
```

**Wire it up in your Page**

```svelte
<script lang="ts">
  import ClippyAssistant, { stepTo } from './ClippyAssistant.svelte';
  let wizardStep = 1;

  function nextStep() {
    wizardStep += 1;
    stepTo(wizardStep);
  }
</script>

<ClippyAssistant on:ready={() => stepTo(1)} />

<div class="wizard">
  {#if wizardStep === 1}
    <!-- Step 1 UI -->
    <button on:click={nextStep}>Set URL â†’</button>
  {:else if wizardStep === 2}
    <!-- Step 2 UI -->
    <button on:click={nextStep}>Set Depth & Filters â†’</button>
  {:else if wizardStep === 3}
    <!-- Step 3 UI -->
    <button on:click={nextStep}>Run Scrape â†’</button>
  {:else}
    <p>ğŸ‰ All steps done! Download your data.</p>
  {/if}
</div>
```

---

9. Updated Prompt for Your Coding Agent

 **Project:** Clippr â€“ The Clippy-Powered Web Scraper

 **Goal:** Build a dual No-Code/Pro-Code scraping platform with Clippy as the interactive assistant.

 **Tech Stack:**

 * **Frontend:** SvelteKit + Tailwind (dark theme, neon accents)
 * **Backend:** Node.js + NestJS/Express + BullMQ
 * **Scraping Engine:** HTTP workers + Puppeteer-core + Proxy Manager
 * **CLI:** TypeScript, published via npm & Homebrew
 * **DB/Storage:** PostgreSQL (metadata), S3/GCS (files), Redis (cache)

  Must-Have Features

 1. **Clippy Assistant**

    * Svelte component wrapping `clippyjs`
    * `stepTo(n)` API for wizard hints, error & success animations
 2. **No-Code Wizard**

    * 4 steps (URL â†’ Depth & Filters â†’ Run â†’ Download)
    * Live preview of selectors & file-type filters
    * Clippy hints at each step
 3. **Pro-Code Mode**

    * Embedded code editor for custom JS/Puppeteer snippets
    * CLI commands: `clippr project create`, `clippr scrape`, `clippr status`, `clippr download`
 4. **Dashboard**

    * Dark-mode globe, live QPS gauge, task list with badges
    * Clippy notifications for job events
 5. **API**

    * REST & gRPC: project/task CRUD, status polling, result fetching
    * API-key auth with scopes

 Deliverables

 * Full repo scaffold (monorepo with `/web`, `/api`, `/cli`)
 * `ClippyAssistant.svelte` + wiring in the wizard
 * CLI tool in TS, published config
 * Backend API modules, DB schema, sample Docker + CI pipelines
 * End-to-end tests for both No-Code and Pro-Code flows

 **Begin** by scaffolding the monorepo and implementing `ClippyAssistant.svelte` exactly as aboveâ€”no placeholders. Ensure Clippy appears and speaks on load.
---