3) HTTPWorker: HTTPWorker.ts (modified)

// api/src/engine/HTTPWorker.ts
import fetch, { RequestInit } from 'node-fetch';
import { WorkerInterface, Job } from './types.js';
import { ProxyManager } from './ProxyManager.js';

export default class HTTPWorker implements WorkerInterface {
  constructor(private proxyManager?: ProxyManager) {}

  async fetch(job: Job): Promise<string> {
    let url = job.url;
    let agent = undefined;
    if (this.proxyManager) {
      const p = this.proxyManager.getNextProxy();
      if (p) {
        // ScraperAPI/Crawlbase require prefixing the URL
        if (p.includes('scraperapi.com') || p.includes('crawlbase.com')) {
          url = p + encodeURIComponent(job.url);
        } else {
          agent = this.proxyManager.createAgent();
        }
      }
    }

    const opts: RequestInit = agent ? { agent } : {};
    const res = await fetch(url, opts);
    if (!res.ok) throw new Error(`HTTP ${res.status}`);
    return await res.text();
  }
}

4) PuppeteerWorker: PuppeteerWorker.ts (modified)

// api/src/engine/PuppeteerWorker.ts
import puppeteer, { Browser } from 'puppeteer-core';
import { WorkerInterface, Job, ProxyConfig } from './types.js';
import { ProxyManager } from './ProxyManager.js';

export class PuppeteerWorker implements WorkerInterface {
  private browser!: Browser;
  private proxyMgr?: ProxyManager;

  constructor(
    private launchOptions: puppeteer.LaunchOptions,
    proxyConfig?: ProxyConfig
  ) {
    if (proxyConfig && proxyConfig.mode !== 'none') {
      this.proxyMgr = new ProxyManager(proxyConfig);
    }
  }

  async init() {
    await this.ensureBrowser();
  }

  private async ensureBrowser() {
    const args = [...(this.launchOptions.args || [])];
    // If custom or scrapeapi/crawlbase, Puppeteer can use --proxy-server
    if (this.proxyMgr) {
      const proxy = this.proxyMgr.getNextProxy();
      if (proxy && !proxy.startsWith('http://api.')) {
        args.push(`--proxy-server=${proxy}`);
      }
    }
    this.browser = await puppeteer.launch({ ...this.launchOptions, args });
  }

  async fetch(job: Job): Promise<string> {
    if (!this.browser) await this.init();
    const page = await this.browser.newPage();
    await page.goto(job.url, { waitUntil: 'networkidle2', timeout: 30000 });
    const content = await page.content();
    await page.close();
    return content;
  }

  async close() {
    if (this.browser) await this.browser.close();
  }
}

5) ScraperService: ScraperService.ts (modified)

// api/src/engine/ScraperService.ts
import HTTPWorker from './HTTPWorker.js';
import { PuppeteerWorker } from './PuppeteerWorker.js';
import { JobConfig, WorkerInterface, Job } from './types.js';
import { ProxyManager } from './ProxyManager.js';

export class ScraperService {
  static async runJob(cfg: JobConfig): Promise<void> {
    // 1) Prepare ProxyManager
    const proxyMgr = new ProxyManager(cfg.proxyConfig);

    // 2) spin up workers
    const workers: WorkerInterface[] = [];
    for (let i = 0; i < cfg.concurrency; i++) {
      if (cfg.renderMode === 'puppeteer') {
        const pw = new PuppeteerWorker(
          { headless: true, args: ['--no-sandbox'] },
          cfg.proxyConfig
        );
        await pw.init();
        workers.push(pw);
      } else {
        workers.push(new HTTPWorker(proxyMgr));
      }
    }

    // 3) dispatch queue as before, using workers...
    // [ queue logic omitted for brevity; same as Sprint 1 ]

    // teardown
    await Promise.all(workers.map(w => w.close?.()));
  }

  // handlePage() unchanged...
}

6) CLI: scrape.ts (modified)

// cli/src/commands/scrape.ts
import { Command } from 'commander';
import { ScraperService } from '../../api/dist/engine/ScraperService.js';

export const scrapeCmd = new Command('scrape')
  .description('Run a scrape job')
  // existing options...
  .option('--proxy-mode <mode>', 'none|scraperapi|crawlbase|custom', 'none')
  .option('--proxy-api-key <key>', 'API key for scraperapi or crawlbase')
  .option('--proxy-list <file>', 'newline-delimited custom proxy list file')
  .action(async (opts) => {
    const proxyConfig = {
      mode: opts.proxyMode as any,
      apiKey: opts.proxyApiKey,
      listFile: opts.proxyList
    };
    const cfg = {
      // ...other cfg...
      proxyConfig
    };
    await ScraperService.runJob(cfg);
  });

7) Unit Test: ProxyManager.test.ts

// api/tests/engine/ProxyManager.test.ts
import { ProxyManager } from '../../src/engine/ProxyManager.js';
import fs from 'fs';
import path from 'path';

describe('ProxyManager', () => {
  const listFile = path.resolve(__dirname, 'proxies.txt');
  beforeAll(() => {
    fs.writeFileSync(listFile, '10.0.0.1:8080\n10.0.0.2:8080\n');
  });
  afterAll(() => fs.unlinkSync(listFile));

  it('rotates custom proxies', () => {
    const pm = new ProxyManager({ mode: 'custom', listFile });
    expect(pm.getNextProxy()).toBe('10.0.0.1:8080');
    expect(pm.getNextProxy()).toBe('10.0.0.2:8080');
    expect(pm.getNextProxy()).toBe('10.0.0.1:8080');
  });

  it('builds ScraperAPI prefix', () => {
    const key = 'ABC';
    const pm = new ProxyManager({ mode: 'scraperapi', apiKey: key });
    const prefix = pm.getNextProxy();
    expect(prefix).toContain(`api.scraperapi.com?api_key=${key}`);
  });

  it('throws if missing credentials', () => {
    expect(() => new ProxyManager({ mode: 'scraperapi' })).toThrow();
  });
});

Create api/tests/engine/proxies.txt with placeholder content if you prefer.
✅ Next Steps

    Merge these changes into your monorepo.

    Install https-proxy-agent:

cd api
npm install https-proxy-agent

Run Tests:

npm test

E2E Check with custom list:

    clippr scrape \
      --start-url http://example.com \
      --render-mode http \
      --proxy-mode custom \
      --proxy-list api/tests/engine/proxies.txt

    Confirm requests go through your local proxies.

Once this is green, we’ll have a full proxy‑pool abstraction that scales across providers, and we can move on to Sprint 3: Scheduler & Watcher.