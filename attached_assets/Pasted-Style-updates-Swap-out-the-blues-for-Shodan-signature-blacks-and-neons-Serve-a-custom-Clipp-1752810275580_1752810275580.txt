Style updates
    Swap out the blues for Shodan signature blacks and neons
    Serve a custom ‚ÄúClippy‚Äù agent 
    Update our Tailwind & CSS theme so everything feels like you pulled up in the hood‚ÄØ‚Äî‚ÄØnot a corporate boardroom

Sprint‚ÄØ2: Distributed Proxies (2‚ÄØWeeks)
‚ú¶ User Stories

    As a pro‚Äëuser, I want to choose a proxy provider (none|scraperapi|crawlbase|custom) via CLI/UI so my scrapes rotate through proxies.

    As a dev, I want a ProxyManager that exposes a common interface for fetching the next proxy and creating HTTP agents.

    As a dev, I want HTTPWorker and PuppeteerWorker to use ProxyManager so all requests go through the proxy pool.

    As a dev, I want unit tests covering both ScraperAPI and a custom proxy‚Äëlist provider.
    As an ops, I want the Docker image to include https-proxy-agent.
‚úî Acceptance Criteria
    CLI/UI accepts:

        --proxy-mode <none|scraperapi|crawlbase|custom>

        --proxy-api-key <key> (for ScraperAPI/Crawlbase)

        --proxy-list <file> (newline‚Äëdelimited list of host:port proxies)

    ProxyManager implements:

        getNextProxy(): string

        createAgent(): HttpAgent

    HTTPWorker.fetch() uses createAgent() behind the scenes.

    PuppeteerWorker launches Chrome with --proxy-server= for each page.

    Tests simulate a dummy custom proxy list and assert that getNextProxy() rotates.

    E2E: run clippr with --proxy-mode custom --proxy-list ./tests/proxies.txt and verify requests go through a local proxy stub.

üìã File Structure

/api
 ‚îî‚îÄ src
    ‚îî‚îÄ engine
       ‚îú‚îÄ ProxyManager.ts
       ‚îú‚îÄ HTTPWorker.ts       (modified)
       ‚îú‚îÄ PuppeteerWorker.ts  (modified)
       ‚îú‚îÄ ScraperService.ts   (modified)
       ‚îî‚îÄ types.ts           (modified)

/cli
 ‚îî‚îÄ src
    ‚îî‚îÄ commands
       ‚îî‚îÄ scrape.ts         (modified)

/api/tests
 ‚îî‚îÄ engine
    ‚îú‚îÄ ProxyManager.test.ts
    ‚îî‚îÄ proxies.txt          (sample proxy list)

1) Types Update: types.ts

// api/src/engine/types.ts
export interface ProxyConfig {
  mode: 'none' | 'scraperapi' | 'crawlbase' | 'custom';
  apiKey?: string;       // for scraperapi / crawlbase
  listFile?: string;     // path to custom list
}

export interface JobConfig {
  startUrl: string;
  depth: number;
  include: string[];
  renderMode: 'http' | 'puppeteer';
  concurrency: number;
  proxyConfig: ProxyConfig;
}

// ... existing Job & WorkerInterface ...

2) ProxyManager: ProxyManager.ts

// api/src/engine/ProxyManager.ts
import fs from 'fs';
import path from 'path';
import { HttpsProxyAgent, HttpsProxyAgentOptions } from 'https-proxy-agent';
import { ProxyConfig } from './types.js';

interface Provider {
  getNext(): string;
  getAgent(): HttpsProxyAgent;
}

class NoneProvider implements Provider {
  getNext() { return ''; }
  getAgent() { return undefined as any; }
}

class ScraperApiProvider implements Provider {
  constructor(private apiKey: string) {}
  getNext() {
    // ScraperAPI uses URL param
    return `http://api.scraperapi.com?api_key=${this.apiKey}&url=`;
  }
  getAgent() {
    // we use direct fetch; agent not needed
    return undefined as any;
  }
}

class CrawlbaseProvider implements Provider {
  constructor(private apiKey: string) {}
  getNext() {
    return `http://api.crawlbase.com/?token=${this.apiKey}&url=`;
  }
  getAgent() { return undefined as any; }
}

class CustomListProvider implements Provider {
  private proxies: string[];
  private idx = 0;
  constructor(listFile: string) {
    const content = fs.readFileSync(path.resolve(listFile), 'utf8');
    this.proxies = content.split(/\r?\n/).filter(Boolean);
  }
  getNext() {
    const proxy = this.proxies[this.idx % this.proxies.length];
    this.idx++;
    return proxy;
  }
  getAgent() {
    const proxy = this.getNext();
    return new HttpsProxyAgent({ host: proxy.split(':')[0], port: parseInt(proxy.split(':')[1],10) } as HttpsProxyAgentOptions);
  }
}

export class ProxyManager {
  private provider: Provider;

  constructor(cfg: ProxyConfig) {
    switch (cfg.mode) {
      case 'scraperapi':
        if (!cfg.apiKey) throw new Error('Missing API key for ScraperAPI');
        this.provider = new ScraperApiProvider(cfg.apiKey);
        break;
      case 'crawlbase':
        if (!cfg.apiKey) throw new Error('Missing API key for Crawlbase');
        this.provider = new CrawlbaseProvider(cfg.apiKey);
        break;
      case 'custom':
        if (!cfg.listFile) throw new Error('Missing listFile for custom proxies');
        this.provider = new CustomListProvider(cfg.listFile);
        break;
      default:
        this.provider = new NoneProvider();
    }
  }

  /** Returns the next proxy URL or prefix */
  getNextProxy(): string {
    return this.provider.getNext();
  }

  /** Returns an HttpAgent to pass into fetch() */
  createAgent() {
    return this.provider.getAgent();
  }
}

    npm install https-proxy-agent

3) HTTPWorker: HTTPWorker.ts (modified)

// api/src/engine/HTTPWorker.ts
import fetch, { RequestInit } from 'node-fetch';
import { WorkerInterface, Job } from './types.js';
import { ProxyManager } from './ProxyManager.js';

export default class HTTPWorker implements WorkerInterface {
  constructor(private proxyManager?: ProxyManager) {}

  async fetch(job: Job): Promise<string> {
    let url = job.url;
    let agent = undefined;
    if (this.proxyManager) {
      const p = this.proxyManager.getNextProxy();
      if (p) {
        // ScraperAPI/Crawlbase require prefixing the URL
        if (p.includes('scraperapi.com') || p.includes('crawlbase.com')) {
          url = p + encodeURIComponent(job.url);
        } else {
          agent = this.proxyManager.createAgent();
        }
      }
    }

    const opts: RequestInit = agent ? { agent } : {};
    const res = await fetch(url, opts);
    if (!res.ok) throw new Error(`HTTP ${res.status}`);
    return await res.text();
  }
}

4) PuppeteerWorker: PuppeteerWorker.ts (modified)

// api/src/engine/PuppeteerWorker.ts
import puppeteer, { Browser } from 'puppeteer-core';
import { WorkerInterface, Job, ProxyConfig } from './types.js';
import { ProxyManager } from './ProxyManager.js';

export class PuppeteerWorker implements WorkerInterface {
  private browser!: Browser;
  private proxyMgr?: ProxyManager;

  constructor(
    private launchOptions: puppeteer.LaunchOptions,
    proxyConfig?: ProxyConfig
  ) {
    if (proxyConfig && proxyConfig.mode !== 'none') {
      this.proxyMgr = new ProxyManager(proxyConfig);
    }
  }

  async init() {
    await this.ensureBrowser();
  }

  private async ensureBrowser() {
    const args = [...(this.launchOptions.args || [])];
    // If custom or scrapeapi/crawlbase, Puppeteer can use --proxy-server
    if (this.proxyMgr) {
      const proxy = this.proxyMgr.getNextProxy();
      if (proxy && !proxy.startsWith('http://api.')) {
        args.push(`--proxy-server=${proxy}`);
      }
    }
    this.browser = await puppeteer.launch({ ...this.launchOptions, args });
  }

  async fetch(job: Job): Promise<string> {
    if (!this.browser) await this.init();
    const page = await this.browser.newPage();
    await page.goto(job.url, { waitUntil: 'networkidle2', timeout: 30000 });
    const content = await page.content();
    await page.close();
    return content;
  }

  async close() {
    if (this.browser) await this.browser.close();
  }
}

5) ScraperService: ScraperService.ts (modified)

// api/src/engine/ScraperService.ts
import HTTPWorker from './HTTPWorker.js';
import { PuppeteerWorker } from './PuppeteerWorker.js';
import { JobConfig, WorkerInterface, Job } from './types.js';
import { ProxyManager } from './ProxyManager.js';

export class ScraperService {
  static async runJob(cfg: JobConfig): Promise<void> {
    // 1) Prepare ProxyManager
    const proxyMgr = new ProxyManager(cfg.proxyConfig);

    // 2) spin up workers
    const workers: WorkerInterface[] = [];
    for (let i = 0; i < cfg.concurrency; i++) {
      if (cfg.renderMode === 'puppeteer') {
        const pw = new PuppeteerWorker(
          { headless: true, args: ['--no-sandbox'] },
          cfg.proxyConfig
        );
        await pw.init();
        workers.push(pw);
      } else {
        workers.push(new HTTPWorker(proxyMgr));
      }
    }

    // 3) dispatch queue as before, using workers...
    // [ queue logic omitted for brevity; same as Sprint 1 ]

    // teardown
    await Promise.all(workers.map(w => w.close?.()));
  }

  // handlePage() unchanged...
}

6) CLI: scrape.ts (modified)

// cli/src/commands/scrape.ts
import { Command } from 'commander';
import { ScraperService } from '../../api/dist/engine/ScraperService.js';

export const scrapeCmd = new Command('scrape')
  .description('Run a scrape job')
  // existing options...
  .option('--proxy-mode <mode>', 'none|scraperapi|crawlbase|custom', 'none')
  .option('--proxy-api-key <key>', 'API key for scraperapi or crawlbase')
  .option('--proxy-list <file>', 'newline-delimited custom proxy list file')
  .action(async (opts) => {
    const proxyConfig = {
      mode: opts.proxyMode as any,
      apiKey: opts.proxyApiKey,
      listFile: opts.proxyList
    };
    const cfg = {
      // ...other cfg...
      proxyConfig
    };
    await ScraperService.runJob(cfg);
  });

7) Unit Test: ProxyManager.test.ts

// api/tests/engine/ProxyManager.test.ts
import { ProxyManager } from '../../src/engine/ProxyManager.js';
import fs from 'fs';
import path from 'path';

describe('ProxyManager', () => {
  const listFile = path.resolve(__dirname, 'proxies.txt');
  beforeAll(() => {
    fs.writeFileSync(listFile, '10.0.0.1:8080\n10.0.0.2:8080\n');
  });
  afterAll(() => fs.unlinkSync(listFile));

  it('rotates custom proxies', () => {
    const pm = new ProxyManager({ mode: 'custom', listFile });
    expect(pm.getNextProxy()).toBe('10.0.0.1:8080');
    expect(pm.getNextProxy()).toBe('10.0.0.2:8080');
    expect(pm.getNextProxy()).toBe('10.0.0.1:8080');
  });

  it('builds ScraperAPI prefix', () => {
    const key = 'ABC';
    const pm = new ProxyManager({ mode: 'scraperapi', apiKey: key });
    const prefix = pm.getNextProxy();
    expect(prefix).toContain(`api.scraperapi.com?api_key=${key}`);
  });

  it('throws if missing credentials', () => {
    expect(() => new ProxyManager({ mode: 'scraperapi' })).toThrow();
  });
});

Create api/tests/engine/proxies.txt with placeholder content if you prefer.
‚úÖ Next Steps

    Merge these changes into your monorepo.

    Install https-proxy-agent:

cd api
npm install https-proxy-agent

Run Tests:

npm test

E2E Check with custom list:

clippr scrape \
  --start-url http://example.com \
  --render-mode http \
  --proxy-mode custom \
  --proxy-list api/tests/engine/proxies.txt

Confirm requests go through your local proxies.
1. Shodan‚ÄëStyle Color Palette
Role	Color HEX	Tailwind Key
Background	#0A0A0D	background: shodan-bg
Surface/Card	#151519	surface: shodan-surface
Text Primary	#E6E6E6	text: shodan-text
Accent (QPS/etc)	#1AFFD5	accent: shodan-accent
Secondary Accent	#00FFC2	accent-2: shodan-accent2
Success	#4AFF4A	success: shodan-success
Warning	#FFCD3C	warning: shodan-warning
Error	#FF4D4F	error: shodan-error
2. Tailwind Configuration

// tailwind.config.cjs
module.exports = {
  darkMode: 'class',
  theme: {
    extend: {
      colors: {
        'shodan-bg':    '#0A0A0D',
        'shodan-surface':'#151519',
        'shodan-text':  '#E6E6E6',
        'shodan-accent':'#1AFFD5',
        'shodan-accent2':'#00FFC2',
        'shodan-success':'#4AFF4A',
        'shodan-warning':'#FFCD3C',
        'shodan-error': '#FF4D4F',
      }
    }
  }
};

Then in your global CSS (e.g. app.css):

:root {
  --bg:    theme('colors.shodan-bg');
  --card:  theme('colors.shodan-surface');
  --text:  theme('colors.shodan-text');
  --accent: theme('colors.shodan-accent');
}

body {
  background-color: var(--bg);
  color: var(--text);
}

Use these classes everywhere:

<div class="bg-shodan-bg text-shodan-text min-h-screen">
  <nav class="bg-shodan-surface p-4"> ‚Ä¶ </nav>
  <button class="bg-shodan-accent hover:bg-shodan-accent2 text-black px-4 py-2 rounded">
    New Scrape
  </button>
</div>

3. Custom ‚ÄúClippy‚Äù Agent (use attached image)

    Add your custom Clippy files into static/agents/StreetClippy/ (sprites .png, JSON manifest, .wav clips).

    Load it by name instead of the vanilla Clippy:

// ClippyAssistant.svelte (modified)
import clippy from 'clippyjs';

onMount(() => {
  // point to your local custom agent path
  clippy.load('StreetClippy', '/agents/StreetClippy/', (agent) => {
    agent.show();
    agent.moveTo(window.innerWidth - 200, window.innerHeight - 200);
    agent.speak("Yo, it‚Äôs your street Clippy‚Äîlet‚Äôs scrape!");
  });
});

This will give you that camo‚ÄëSupreme‚Äëboot look straight out of your upload, rather than the bland Microsoft clip.
4. Example SvelteKit Layout

<!-- src/routes/+layout.svelte -->
<script>
  import ClippyAssistant from '$lib/components/ClippyAssistant.svelte';
</script>

<div class="flex h-screen">
  <aside class="w-60 bg-shodan-surface text-shodan-text">
    <!-- your nav -->
  </aside>
  <main class="flex-1 bg-shodan-bg p-6">
    <slot />
    <ClippyAssistant />
  </main>
</div>

And in your components:

<!-- NewScrapeButton.svelte -->
<button
  class="bg-shodan-accent hover:bg-shodan-accent2 text-black font-bold py-2 px-4 rounded shadow-md"
>
  New Scrape
</button>

Wrap‚Äëup

    Tailwind theme now uses true Shodan blacks & neons

    Body + cards adopt that dark‚Äësurface look

    StreetClippy agent skin lives in your static/agents/StreetClippy folder

    Svelte layout updated to use the new palette everywhere
Phase‚ÄØ2 Roadmap (8‚ÄØSprints, 2‚ÄØweeks each)
Sprint	Focus	Deliverables
1	Hybrid Rendering	HTTP‚ÄØworkers + optional Puppeteer mode; --render-mode flag in CLI; engine tests
2	Distributed Proxies	Proxy‚Äëpool abstraction; integrations with ScraperAPI & Crawlbase; per‚Äëdomain rotation rules
3	Scheduler & Watcher	Recurring jobs UI & API; ‚Äúonly‚Äëwhen‚Äëchanged‚Äù watcher; webhook triggers
4	Full REST & gRPC API	Project/task CRUD; status polling; result endpoints; scoped API‚Äëkeys
5	Plugin Marketplace	Auth‚Äëmodules (OAuth, SAML); parser‚Äëplugin hooks; UI to install/uninstall plugins
6	3D Globe Dashboard	Three.js earth view; live QPS gauge; region‚Äëcluster pins; task‚Äëstatus overlays
7	Analytics Tab	Charts (requests/time, latency, errors heatmap) built with Chart.js or Recharts
8	Team & RBAC	User/role management; project‚Äësharing UI; audit logs
Sprint‚ÄØ1: Hybrid Rendering
1. Extend CLI to accept --render-mode

// cli/src/commands/scrape.ts
import { Command } from 'commander';
const program = new Command();

program
  .name('clippr')
  .command('scrape')
  .description('Run a scrape job')
  .requiredOption('--start-url <url>')
  .option('--depth <n>', 'crawl depth', '2')
  .option('--include <types>', 'comma‚Äëseparated file types', 'html')
  .option('--render-mode <mode>', 'http or puppeteer', 'http')
  .option('--concurrency <n>', 'number of workers', '5')
  .action(async (opts) => {
    const config = {
      startUrl: opts.startUrl,
      depth: parseInt(opts.depth, 10),
      include: opts.include.split(','),
      renderMode: opts.renderMode as 'http' | 'puppeteer',
      concurrency: parseInt(opts.concurrency, 10),
    };
    // Pass config into our ScraperService
    await ScraperService.runJob(config);
  });

program.parse(process.argv);

2. Implement PuppeteerWorker

// api/src/engine/PuppeteerWorker.ts
import puppeteer, { Browser, Page } from 'puppeteer-core';
import { WorkerInterface, Job } from './types';

export class PuppeteerWorker implements WorkerInterface {
  private browser!: Browser;

  constructor(private launchOptions: puppeteer.LaunchOptions) {}

  async init() {
    this.browser = await puppeteer.launch(this.launchOptions);
  }

  async fetch(job: Job): Promise<string> {
    const page: Page = await this.browser.newPage();
    await page.goto(job.url, { waitUntil: 'networkidle2', timeout: 30000 });
    const content = await page.content();
    await page.close();
    return content;
  }

  async close() {
    if (this.browser) {
      await this.browser.close();
    }
  }
}

3. Wire up in ScraperService

// api/src/engine/ScraperService.ts
import HTTPWorker from './HTTPWorker';
import { PuppeteerWorker } from './PuppeteerWorker';
import { JobConfig, JobResult } from './types';

export class ScraperService {
  static async runJob(cfg: JobConfig): Promise<void> {
    const workers = [];
    if (cfg.renderMode === 'puppeteer') {
      const puppeteerWorker = new PuppeteerWorker({
        headless: true,
        args: ['--no-sandbox', '--disable-setuid-sandbox']
      });
      await puppeteerWorker.init();
      workers.push(puppeteerWorker);
    } else {
      workers.push(new HTTPWorker());
    }

    // simple round‚Äërobin scheduler
    const queue: string[] = [cfg.startUrl];
    while (queue.length) {
      const url = queue.shift()!;
      const worker = workers.shift()!;
      try {
        const html = await worker.fetch({ url });
        await this.handlePage(html, url, cfg);
      } catch (err) {
        console.error(`‚úñ fetch failed: ${url}`, err);
      } finally {
        workers.push(worker);
      }
    }

    // teardown
    await Promise.all(workers.map(w => 'close' in w ? w.close() : Promise.resolve()));
  }

  private static async handlePage(html: string, url: string, cfg: JobConfig): Promise<void> {
    // existing logic: parse out links up to cfg.depth, save files matching cfg.include, etc.
  }
}

4. Update Types

// api/src/engine/types.ts
export interface JobConfig {
  startUrl: string;
  depth: number;
  include: string[];
  renderMode: 'http' | 'puppeteer';
  concurrency: number;
}

export interface Job {
  url: string;
}

export interface WorkerInterface {
  fetch(job: Job): Promise<string>;
  close?(): Promise<void>;
}

5. Test & Validate

    Unit Tests:

        HTTPWorker still passes all existing tests.

        Add tests that run with renderMode: 'puppeteer' against a simple local HTML server.

    E2E:

    clippr scrape \
      --start-url http://localhost:3000/test.html \
      --depth 1 \
      --include html \
      --render-mode puppeteer \
      --concurrency 2

    Verify it saves the HTML to disk.

Sprint‚ÄØ1: Hybrid Rendering (2‚ÄØWeeks)
‚ú¶ User Stories

    As a pro‚Äëuser, I want to choose between http and puppeteer render modes via CLI so I can scrape both static and JS‚Äëheavy pages.

    As a dev, I want a PuppeteerWorker that implements our WorkerInterface so we can spin up headless Chrome when needed.

    As a dev, I want existing HTTPWorker to remain untouched and tests to pass.

    As a maintainer, I want our Docker image to ship with Chromium so puppeteer-core can launch.

‚úî Acceptance Criteria

    clippr scrape --render-mode puppeteer spins up Chrome, fetches page, and returns HTML.

    clippr scrape --render-mode http fallback still works exactly as before.

    Unit tests cover both workers.

    A local E2E run against a tiny test server saves HTML to disk.

    Docker image installs Chromium and fonts.

üìã Tasks & File Structure

/cli
 ‚îî‚îÄ src
    ‚îú‚îÄ index.ts
    ‚îî‚îÄ commands
       ‚îî‚îÄ scrape.ts

/api
 ‚îî‚îÄ src
    ‚îî‚îÄ engine
       ‚îú‚îÄ HTTPWorker.ts
       ‚îú‚îÄ PuppeteerWorker.ts
       ‚îú‚îÄ ScraperService.ts
       ‚îî‚îÄ types.ts

 ‚îî‚îÄ tests
    ‚îî‚îÄ engine
       ‚îî‚îÄ PuppeteerWorker.test.ts

 ‚îî‚îÄ Dockerfile

1) CLI: scrape.ts

// cli/src/commands/scrape.ts
import { Command } from 'commander';
import { ScraperService } from '../../api/dist/engine/ScraperService.js';

export const scrapeCmd = new Command('scrape')
  .description('Run a scrape job')
  .requiredOption('--start-url <url>', 'Starting URL')
  .option('--depth <n>', 'crawl depth', '2')
  .option('--include <types>', 'comma‚Äëseparated file types', 'html')
  .option('--render-mode <mode>', 'http or puppeteer', 'http')
  .option('--concurrency <n>', 'number of workers', '5')
  .action(async (opts) => {
    const cfg = {
      startUrl: opts.startUrl,
      depth:   parseInt(opts.depth, 10),
      include: opts.include.split(','),
      renderMode: opts.renderMode as 'http'|'puppeteer',
      concurrency: parseInt(opts.concurrency, 10),
    };
    await ScraperService.runJob(cfg);
  });

// cli/src/index.ts
import { Command } from 'commander';
import { scrapeCmd } from './commands/scrape.js';

const program = new Command();
program
  .name('clippr')
  .version('0.1.0')
  .addCommand(scrapeCmd);

program.parse(process.argv);

2) Engine Types: types.ts

// api/src/engine/types.ts
export interface JobConfig {
  startUrl: string;
  depth: number;
  include: string[];
  renderMode: 'http' | 'puppeteer';
  concurrency: number;
}

export interface Job {
  url: string;
}

export interface WorkerInterface {
  fetch(job: Job): Promise<string>;
  close?(): Promise<void>;
}

3) HTTPWorker (unchanged): HTTPWorker.ts

// api/src/engine/HTTPWorker.ts
import fetch from 'node-fetch';
import { WorkerInterface, Job } from './types.js';

export default class HTTPWorker implements WorkerInterface {
  async fetch(job: Job): Promise<string> {
    const res = await fetch(job.url, { timeout: 15000 });
    if (!res.ok) throw new Error(`HTTP ${res.status}`);
    return await res.text();
  }
}

4) PuppeteerWorker: PuppeteerWorker.ts

// api/src/engine/PuppeteerWorker.ts
import puppeteer, { Browser } from 'puppeteer-core';
import { WorkerInterface, Job } from './types.js';

export class PuppeteerWorker implements WorkerInterface {
  private browser!: Browser;

  constructor(private launchOptions: puppeteer.LaunchOptions) {}

  async init() {
    this.browser = await puppeteer.launch(this.launchOptions);
  }

  async fetch(job: Job): Promise<string> {
    const page = await this.browser.newPage();
    await page.goto(job.url, { waitUntil: 'networkidle2', timeout: 30000 });
    const content = await page.content();
    await page.close();
    return content;
  }

  async close() {
    if (this.browser) await this.browser.close();
  }
}

5) ScraperService: ScraperService.ts

// api/src/engine/ScraperService.ts
import HTTPWorker from './HTTPWorker.js';
import { PuppeteerWorker } from './PuppeteerWorker.js';
import { JobConfig, WorkerInterface, Job } from './types.js';
import path from 'path';
import fs from 'fs/promises';

export class ScraperService {
  static async runJob(cfg: JobConfig): Promise<void> {
    // 1) spin up workers
    const workers: WorkerInterface[] = [];
    if (cfg.renderMode === 'puppeteer') {
      const pw = new PuppeteerWorker({ headless: true, args: ['--no-sandbox'] });
      await pw.init();
      workers.push(pw);
    } else {
      workers.push(new HTTPWorker());
    }

    // 2) simple queue
    const queue: Job[] = [{ url: cfg.startUrl }];
    const seen = new Set<string>();

    while (queue.length) {
      const job = queue.shift()!;
      if (seen.has(job.url)) continue;
      seen.add(job.url);

      const worker = workers.shift()!;
      try {
        const html = await worker.fetch(job);
        await this.handlePage(html, job.url, cfg);
        // parse links and enqueue (depth logic omitted for brevity)
      } catch (err) {
        console.error(`‚úñ fetch failed for ${job.url}`, err);
      } finally {
        workers.push(worker);
      }
    }

    // 3) teardown
    await Promise.all(workers.map(w => w.close ? w.close() : Promise.resolve()));
  }

  private static async handlePage(html: string, url: string, cfg: JobConfig) {
    // save to disk if matches include types
    const ext = path.extname(url) || '.html';
    if (cfg.include.includes(ext.replace('.', ''))) {
      const fname = Buffer.from(url).toString('base64') + ext;
      await fs.mkdir('output', { recursive: true });
      await fs.writeFile(path.join('output', fname), html, 'utf8');
      console.log(`[+] Saved ${fname}`);
    }
  }
}

6) Unit Test: PuppeteerWorker.test.ts

// api/tests/engine/PuppeteerWorker.test.ts
import http from 'http';
import { PuppeteerWorker } from '../../src/engine/PuppeteerWorker.js';

describe('PuppeteerWorker', () => {
  let server: http.Server;
  const port = 4001;
  const html = '<html><body><h1>Test</h1></body></html>';

  beforeAll(done => {
    server = http.createServer((req, res) => {
      res.end(html);
    }).listen(port, done);
  });

  afterAll(done => server.close(done));

  it('fetches HTML content via puppeteer', async () => {
    const w = new PuppeteerWorker({
      headless: true,
      args: ['--no-sandbox', '--disable-setuid-sandbox']
    });
    await w.init();
    const content = await w.fetch({ url: `http://localhost:${port}` });
    expect(content).toContain('<h1>Test</h1>');
    await w.close();
  });
});

7) Dockerfile

# api/Dockerfile
FROM node:18‚Äëbuster

# install Chrome for puppeteer-core
RUN apt‚Äëget update \
 && apt‚Äëget install ‚Äëy wget gnupg libgbm-dev \
 && wget -qO - https://dl‚Äëgoogle.com/linux/linux_signing_key.pub | apt‚Äëkey add - \
 && echo "deb [arch=amd64] http://dl‚Äëgoogle.com/linux/chrome/deb/ stable main" > /etc/apt/sources.list.d/google-chrome.list \
 && apt‚Äëget update \
 && apt‚Äëget install -y google-chrome-stable \
 && rm -rf /var/lib/apt/lists/*

WORKDIR /usr/src/app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

CMD ["node", "dist/server.js"]

‚úÖ Next Steps

    Merge this sprint into your monorepo.

    Run npm test to validate both HTTP & Puppeteer workers.

    Build & run Docker:

docker build -t clippr-api ./api
docker run --rm clippr-api clippr scrape --start-url http://example.com --render-mode puppeteer

Verify that output/ contains the fetched HTML.

Sprint‚ÄØ2: Distributed Proxies (2‚ÄØWeeks)
‚ú¶ User Stories

    As a pro‚Äëuser, I want to choose a proxy provider (none|scraperapi|crawlbase|custom) via CLI/UI so my scrapes rotate through proxies.

    As a dev, I want a ProxyManager that exposes a common interface for fetching the next proxy and creating HTTP agents.

    As a dev, I want HTTPWorker and PuppeteerWorker to use ProxyManager so all requests go through the proxy pool.

    As a dev, I want unit tests covering both ScraperAPI and a custom proxy‚Äëlist provider.

    As an ops, I want the Docker image to include https-proxy-agent.

‚úî Acceptance Criteria

    CLI/UI accepts:

        --proxy-mode <none|scraperapi|crawlbase|custom>

        --proxy-api-key <key> (for ScraperAPI/Crawlbase)

        --proxy-list <file> (newline‚Äëdelimited list of host:port proxies)

    ProxyManager implements:

        getNextProxy(): string

        createAgent(): HttpAgent

    HTTPWorker.fetch() uses createAgent() behind the scenes.

    PuppeteerWorker launches Chrome with --proxy-server= for each page.

    Tests simulate a dummy custom proxy list and assert that getNextProxy() rotates.

    E2E: run clippr with --proxy-mode custom --proxy-list ./tests/proxies.txt and verify requests go through a local proxy stub.

üìã File Structure

/api
 ‚îî‚îÄ src
    ‚îî‚îÄ engine
       ‚îú‚îÄ ProxyManager.ts
       ‚îú‚îÄ HTTPWorker.ts       (modified)
       ‚îú‚îÄ PuppeteerWorker.ts  (modified)
       ‚îú‚îÄ ScraperService.ts   (modified)
       ‚îî‚îÄ types.ts           (modified)

/cli
 ‚îî‚îÄ src
    ‚îî‚îÄ commands
       ‚îî‚îÄ scrape.ts         (modified)

/api/tests
 ‚îî‚îÄ engine
    ‚îú‚îÄ ProxyManager.test.ts
    ‚îî‚îÄ proxies.txt          (sample proxy list)

1) Types Update: types.ts

// api/src/engine/types.ts
export interface ProxyConfig {
  mode: 'none' | 'scraperapi' | 'crawlbase' | 'custom';
  apiKey?: string;       // for scraperapi / crawlbase
  listFile?: string;     // path to custom list
}

export interface JobConfig {
  startUrl: string;
  depth: number;
  include: string[];
  renderMode: 'http' | 'puppeteer';
  concurrency: number;
  proxyConfig: ProxyConfig;
}

// ... existing Job & WorkerInterface ...

2) ProxyManager: ProxyManager.ts

// api/src/engine/ProxyManager.ts
import fs from 'fs';
import path from 'path';
import { HttpsProxyAgent, HttpsProxyAgentOptions } from 'https-proxy-agent';
import { ProxyConfig } from './types.js';

interface Provider {
  getNext(): string;
  getAgent(): HttpsProxyAgent;
}

class NoneProvider implements Provider {
  getNext() { return ''; }
  getAgent() { return undefined as any; }
}

class ScraperApiProvider implements Provider {
  constructor(private apiKey: string) {}
  getNext() {
    // ScraperAPI uses URL param
    return `http://api.scraperapi.com?api_key=${this.apiKey}&url=`;
  }
  getAgent() {
    // we use direct fetch; agent not needed
    return undefined as any;
  }
}

class CrawlbaseProvider implements Provider {
  constructor(private apiKey: string) {}
  getNext() {
    return `http://api.crawlbase.com/?token=${this.apiKey}&url=`;
  }
  getAgent() { return undefined as any; }
}

class CustomListProvider implements Provider {
  private proxies: string[];
  private idx = 0;
  constructor(listFile: string) {
    const content = fs.readFileSync(path.resolve(listFile), 'utf8');
    this.proxies = content.split(/\r?\n/).filter(Boolean);
  }
  getNext() {
    const proxy = this.proxies[this.idx % this.proxies.length];
    this.idx++;
    return proxy;
  }
  getAgent() {
    const proxy = this.getNext();
    return new HttpsProxyAgent({ host: proxy.split(':')[0], port: parseInt(proxy.split(':')[1],10) } as HttpsProxyAgentOptions);
  }
}

export class ProxyManager {
  private provider: Provider;

  constructor(cfg: ProxyConfig) {
    switch (cfg.mode) {
      case 'scraperapi':
        if (!cfg.apiKey) throw new Error('Missing API key for ScraperAPI');
        this.provider = new ScraperApiProvider(cfg.apiKey);
        break;
      case 'crawlbase':
        if (!cfg.apiKey) throw new Error('Missing API key for Crawlbase');
        this.provider = new CrawlbaseProvider(cfg.apiKey);
        break;
      case 'custom':
        if (!cfg.listFile) throw new Error('Missing listFile for custom proxies');
        this.provider = new CustomListProvider(cfg.listFile);
        break;
      default:
        this.provider = new NoneProvider();
    }
  }

  /** Returns the next proxy URL or prefix */
  getNextProxy(): string {
    return this.provider.getNext();
  }

  /** Returns an HttpAgent to pass into fetch() */
  createAgent() {
    return this.provider.getAgent();
  }
}

    npm install https-proxy-agent

3) HTTPWorker: HTTPWorker.ts (modified)

// api/src/engine/HTTPWorker.ts
import fetch, { RequestInit } from 'node-fetch';
import { WorkerInterface, Job } from './types.js';
import { ProxyManager } from './ProxyManager.js';

export default class HTTPWorker implements WorkerInterface {
  constructor(private proxyManager?: ProxyManager) {}

  async fetch(job: Job): Promise<string> {
    let url = job.url;
    let agent = undefined;
    if (this.proxyManager) {
      const p = this.proxyManager.getNextProxy();
      if (p) {
        // ScraperAPI/Crawlbase require prefixing the URL
        if (p.includes('scraperapi.com') || p.includes('crawlbase.com')) {
          url = p + encodeURIComponent(job.url);
        } else {
          agent = this.proxyManager.createAgent();
        }
      }
    }

    const opts: RequestInit = agent ? { agent } : {};
    const res = await fetch(url, opts);
    if (!res.ok) throw new Error(`HTTP ${res.status}`);
    return await res.text();
  }
}

4) PuppeteerWorker: PuppeteerWorker.ts (modified)

// api/src/engine/PuppeteerWorker.ts
import puppeteer, { Browser } from 'puppeteer-core';
import { WorkerInterface, Job, ProxyConfig } from './types.js';
import { ProxyManager } from './ProxyManager.js';

export class PuppeteerWorker implements WorkerInterface {
  private browser!: Browser;
  private proxyMgr?: ProxyManager;

  constructor(
    private launchOptions: puppeteer.LaunchOptions,
    proxyConfig?: ProxyConfig
  ) {
    if (proxyConfig && proxyConfig.mode !== 'none') {
      this.proxyMgr = new ProxyManager(proxyConfig);
    }
  }

  async init() {
    await this.ensureBrowser();
  }

  private async ensureBrowser() {
    const args = [...(this.launchOptions.args || [])];
    // If custom or scrapeapi/crawlbase, Puppeteer can use --proxy-server
    if (this.proxyMgr) {
      const proxy = this.proxyMgr.getNextProxy();
      if (proxy && !proxy.startsWith('http://api.')) {
        args.push(`--proxy-server=${proxy}`);
      }
    }
    this.browser = await puppeteer.launch({ ...this.launchOptions, args });
  }

  async fetch(job: Job): Promise<string> {
    if (!this.browser) await this.init();
    const page = await this.browser.newPage();
    await page.goto(job.url, { waitUntil: 'networkidle2', timeout: 30000 });
    const content = await page.content();
    await page.close();
    return content;
  }

  async close() {
    if (this.browser) await this.browser.close();
  }
}

5) ScraperService: ScraperService.ts (modified)

// api/src/engine/ScraperService.ts
import HTTPWorker from './HTTPWorker.js';
import { PuppeteerWorker } from './PuppeteerWorker.js';
import { JobConfig, WorkerInterface, Job } from './types.js';
import { ProxyManager } from './ProxyManager.js';

export class ScraperService {
  static async runJob(cfg: JobConfig): Promise<void> {
    // 1) Prepare ProxyManager
    const proxyMgr = new ProxyManager(cfg.proxyConfig);

    // 2) spin up workers
    const workers: WorkerInterface[] = [];
    for (let i = 0; i < cfg.concurrency; i++) {
      if (cfg.renderMode === 'puppeteer') {
        const pw = new PuppeteerWorker(
          { headless: true, args: ['--no-sandbox'] },
          cfg.proxyConfig
        );
        await pw.init();
        workers.push(pw);
      } else {
        workers.push(new HTTPWorker(proxyMgr));
      }
    }

    // 3) dispatch queue as before, using workers...
    // [ queue logic omitted for brevity; same as Sprint 1 ]

    // teardown
    await Promise.all(workers.map(w => w.close?.()));
  }

  // handlePage() unchanged...
}

6) CLI: scrape.ts (modified)

// cli/src/commands/scrape.ts
import { Command } from 'commander';
import { ScraperService } from '../../api/dist/engine/ScraperService.js';

export const scrapeCmd = new Command('scrape')
  .description('Run a scrape job')
  // existing options...
  .option('--proxy-mode <mode>', 'none|scraperapi|crawlbase|custom', 'none')
  .option('--proxy-api-key <key>', 'API key for scraperapi or crawlbase')
  .option('--proxy-list <file>', 'newline-delimited custom proxy list file')
  .action(async (opts) => {
    const proxyConfig = {
      mode: opts.proxyMode as any,
      apiKey: opts.proxyApiKey,
      listFile: opts.proxyList
    };
    const cfg = {
      // ...other cfg...
      proxyConfig
    };
    await ScraperService.runJob(cfg);
  });

7) Unit Test: ProxyManager.test.ts

// api/tests/engine/ProxyManager.test.ts
import { ProxyManager } from '../../src/engine/ProxyManager.js';
import fs from 'fs';
import path from 'path';

describe('ProxyManager', () => {
  const listFile = path.resolve(__dirname, 'proxies.txt');
  beforeAll(() => {
    fs.writeFileSync(listFile, '10.0.0.1:8080\n10.0.0.2:8080\n');
  });
  afterAll(() => fs.unlinkSync(listFile));

  it('rotates custom proxies', () => {
    const pm = new ProxyManager({ mode: 'custom', listFile });
    expect(pm.getNextProxy()).toBe('10.0.0.1:8080');
    expect(pm.getNextProxy()).toBe('10.0.0.2:8080');
    expect(pm.getNextProxy()).toBe('10.0.0.1:8080');
  });

  it('builds ScraperAPI prefix', () => {
    const key = 'ABC';
    const pm = new ProxyManager({ mode: 'scraperapi', apiKey: key });
    const prefix = pm.getNextProxy();
    expect(prefix).toContain(`api.scraperapi.com?api_key=${key}`);
  });

  it('throws if missing credentials', () => {
    expect(() => new ProxyManager({ mode: 'scraperapi' })).toThrow();
  });
});

Create api/tests/engine/proxies.txt with placeholder content if you prefer.
‚úÖ Next Steps

    Merge these changes into your monorepo.

    Install https-proxy-agent:

cd api
npm install https-proxy-agent

Run Tests:

npm test

E2E Check with custom list:

clippr scrape \
  --start-url http://example.com \
  --render-mode http \
  --proxy-mode custom \
  --proxy-list api/tests/engine/proxies.txt

Confirm requests go through your local proxies.